## Project Title
This project demonstrates an **End-to-End Data Engineering Pipeline** built on Microsoft Azure. 

## Overview
Data was extracted from Kaggle, processed and transformed using Azure Databricks (PySpark), and stored in Azure Data Lake Gen2. The processed data was then loaded into Azure Synapse Analytics for analysis and querying.
Power BI was connected to Synapse to verify the connection and preview the dataset for reporting purposes.

## Architecture Diagram (Project Flow)
Kaggle Dataset
     ↓
GitHub Repository
     ↓
Azure Data Factory
     ↓
Raw Layer (Data Lake Gen2)
     ↓
Azure Databricks (PySpark)
     ↓
Transformation Layer (Cleaned Data)
     ↓
Serving Layer (Curated Data)
     ↓
Azure Synapse Analytics
     ↓
Power BI (Connected)

### Data Lake Architecture
The Azure Data Lake Gen2 is organized into three layers to manage data efficiently throughout its lifecycle:
- **Raw Layer**: Stores original data ingested from Kaggle via Azure Data Factory.
- **Transformation Layer**: Contains cleaned and processed data generated by Azure Databricks (PySpark).
- **Serving Layer**: Stores curated, analytics-ready data used by Azure Synapse Analytics and connected to Power BI for visualization.

## Tools & Technologies
- **Azure Data Factory (ADF)** – Data ingestion & pipeline orchestration
- **Azure Data Lake Gen2** – Data storage (Raw → Transformed → Serving layers)
- **Azure Databricks (PySpark)** – Data cleaning, transformation, and preparation
- **Azure Synapse Analytics** – Data warehouse, SQL views & analytics
- **Power BI** – Reporting and visualization
- **GitHub** – Version control and project hosting
- **Kaggle** – Dataset source

## Dataset
- **Source** [Kaggle - *Adventure Works*] (https://www.kaggle.com/datasets/ukveteran/adventure-works)
- **Description** - The Adventure Works dataset is a sample business database that simulates a global manufacturing company specializing in the sale of bicycles and related products. It includes data on sales, customers, products, employees, and territories.
- **Size** - Approximately 60–100 MB in total, consisting of multiple CSV files representing different business tables such as Sales, Product, Customer, Employee, and Territory.
- **Usage** - Used as the source data for building an end-to-end Azure Data Engineering pipeline — from ingestion to transformation and serving.
- **License** - Publicly available on Kaggle under the CC0: Public Domain license.

## Project Workflow
### 1️⃣ Data Source
- Dataset: [Kaggle – Adventure Works](https://www.kaggle.com/datasets/ukveteran/adventure-works)  
- The dataset contains multiple CSV files (Sales, Product, Customer, etc.).
  
### 2️⃣ Data Ingestion
- Used **Azure Data Factory (ADF)** dynamic pipelines to automate data movement.  
- Ingested CSV files → converted to JSON → stored in **Raw Layer (Bronze Zone)** of **Azure Data Lake Gen2**.  
- Configured pipelines to dynamically process multiple files for scalability.  

### 3️⃣ Data Transformation
- Utilized **Azure Databricks (PySpark)** notebooks to clean and transform data:
  - Handled missing values and duplicates  
  - Standardized formats (dates, numerical columns, etc.)  
  - Modified schema and column names for consistency  
- Saved cleaned data to **Transformation Layer (Silver Zone)** in Data Lake Gen2.  

### 4️⃣ Data Serving
- Loaded curated, analytics-ready data into **Azure Synapse Analytics**.  
- Created **SQL views** for organized, query-ready datasets.  

### 5️⃣ Data Connection (Power BI)
- Connected **Power BI** to **Azure Synapse Analytics** to verify data availability and table relationships.  
- Data ready for visualization and further reporting.  






























## Project Title
This project demonstrates an **End-to-End Data Engineering Pipeline** built on Microsoft Azure. 

## Overview
Data was extracted from Kaggle, processed and transformed using Azure Databricks (PySpark), and stored in Azure Data Lake Gen2. The processed data was then loaded into Azure Synapse Analytics for analysis and querying.
Power BI was connected to Synapse to verify the connection and preview the dataset for reporting purposes.

## Architecture Diagram (Project Flow)
Kaggle Dataset
     ↓
GitHub Repository
     ↓
Azure Data Factory
     ↓
Raw Layer (Data Lake Gen2)
     ↓
Azure Databricks (PySpark)
     ↓
Transformation Layer (Cleaned Data)
     ↓
Serving Layer (Curated Data)
     ↓
Azure Synapse Analytics
     ↓
Power BI (Connected)

### Data Lake Architecture
The Azure Data Lake Gen2 is organized into three layers to manage data efficiently throughout its lifecycle:
- **Raw Layer**: Stores original data ingested from Kaggle via Azure Data Factory.
- **Transformation Layer**: Contains cleaned and processed data generated by Azure Databricks (PySpark).
- **Serving Layer**: Stores curated, analytics-ready data used by Azure Synapse Analytics and connected to Power BI for visualization.



## Tools & Technologies
- **Azure Data Factory (ADF)** – Data ingestion & pipeline orchestration
- **Azure Data Lake Gen2** – Data storage (Raw → Transformed → Serving layers)
- **Azure Databricks (PySpark)** – Data cleaning, transformation, and preparation
- **Azure Synapse Analytics** – Data warehouse, SQL views & analytics
- **Power BI** – Reporting and visualization
- **GitHub** – Version control and project hosting
- **Kaggle** – Dataset source

## Dataset
- **Source** [Kaggle - *Adventure Works*] (https://www.kaggle.com/datasets/ukveteran/adventure-works)
- **Description** - The Adventure Works dataset is a sample business database that simulates a global manufacturing company specializing in the sale of bicycles and related products. It includes data on sales, customers, products, employees, and territories.
- **Size** - Approximately 60–100 MB in total, consisting of multiple CSV files representing different business tables such as Sales, Product, Customer, Employee, and Territory.
- **Usage** - Used as the source data for building an end-to-end Azure Data Engineering pipeline — from ingestion to transformation and serving.
- **License** - Publicly available on Kaggle under the CC0: Public Domain license.

